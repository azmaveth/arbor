#!/bin/bash
set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}âš¡ Running Arbor performance benchmarks...${NC}"
echo

# Parse command line arguments
SUITE="all"
OUTPUT_DIR="benchmarks/results"
FORMAT="console"
SAVE_RESULTS=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --suite)
            SUITE="$2"
            shift 2
            ;;
        --output)
            OUTPUT_DIR="$2"
            SAVE_RESULTS=true
            shift 2
            ;;
        --format)
            FORMAT="$2"
            shift 2
            ;;
        --save)
            SAVE_RESULTS=true
            shift
            ;;
        --help)
            echo "Usage: $0 [options]"
            echo "Options:"
            echo "  --suite SUITE    Benchmark suite to run (default: all)"
            echo "                   Available: all, contracts, messaging, persistence"
            echo "  --output DIR     Save results to directory (default: benchmarks/results)"
            echo "  --format FORMAT  Output format: console, html, json (default: console)"
            echo "  --save           Save results to files"
            echo "  --help           Show this help"
            echo
            echo "Examples:"
            echo "  $0                           # Run all benchmarks"
            echo "  $0 --suite messaging        # Run only messaging benchmarks"
            echo "  $0 --save --format html     # Save HTML report"
            exit 0
            ;;
        *)
            echo -e "${RED}Unknown option: $1${NC}"
            exit 1
            ;;
    esac
done

# Ensure we're in production-like environment for benchmarks
export MIX_ENV=prod

echo -e "${YELLOW}ğŸ—ï¸  Compiling for production environment...${NC}"
mix deps.get --only=prod > /dev/null 2>&1
mix compile > /dev/null 2>&1
echo

# Create output directory if needed
if [ "$SAVE_RESULTS" = true ]; then
    mkdir -p "$OUTPUT_DIR"
    echo -e "${YELLOW}ğŸ“ Results will be saved to: $OUTPUT_DIR${NC}"
    echo
fi

# Function to run benchmark suite
run_benchmark() {
    local suite_name=$1
    local description=$2
    local code=$3
    
    echo -e "${YELLOW}ğŸ“Š Running $description...${NC}"
    
    # Create temporary benchmark file
    local bench_file="/tmp/arbor_bench_${suite_name}.exs"
    
    cat > "$bench_file" << EOF
# Arbor $description
# Generated by scripts/benchmark.sh

Mix.install([{:benchee, "~> 1.3"}])

$code
EOF
    
    if [ "$SAVE_RESULTS" = true ]; then
        local output_file="$OUTPUT_DIR/${suite_name}_$(date +%Y%m%d_%H%M%S)"
        case "$FORMAT" in
            html)
                elixir "$bench_file" --formatters Benchee.Formatters.HTML --html file: "${output_file}.html"
                echo -e "${GREEN}ğŸ“„ HTML report: ${output_file}.html${NC}"
                ;;
            json)
                elixir "$bench_file" --formatters Benchee.Formatters.JSON --json file: "${output_file}.json"
                echo -e "${GREEN}ğŸ“„ JSON report: ${output_file}.json${NC}"
                ;;
            *)
                elixir "$bench_file" | tee "${output_file}.txt"
                echo -e "${GREEN}ğŸ“„ Results saved: ${output_file}.txt${NC}"
                ;;
        esac
    else
        elixir "$bench_file"
    fi
    
    rm -f "$bench_file"
    echo
}

# Start timestamp
start_time=$(date +%s)

# Contract validation benchmarks
if [ "$SUITE" = "all" ] || [ "$SUITE" = "contracts" ]; then
    run_benchmark "contracts" "Contract Validation Benchmarks" '
# Benchmark contract validation performance
Benchee.run(%{
  "simple_struct_creation" => fn ->
    %{id: "test", name: "benchmark", value: 42}
  end,
  "map_validation" => fn ->
    data = %{id: "test", name: "benchmark", value: 42}
    case data do
      %{id: id, name: name} when is_binary(id) and is_binary(name) -> :ok
      _ -> :error
    end
  end,
  "nested_map_creation" => fn ->
    %{
      agent: %{id: "agent_1", type: "coordinator"},
      capability: %{resource: "test", action: "read"},
      metadata: %{timestamp: System.system_time(), user: "test"}
    }
  end
}, time: 5, memory_time: 2)
'
fi

# Messaging benchmarks
if [ "$SUITE" = "all" ] || [ "$SUITE" = "messaging" ]; then
    run_benchmark "messaging" "Message Passing Benchmarks" '
# Benchmark message passing performance
defmodule BenchAgent do
  use GenServer
  
  def start_link(_), do: GenServer.start_link(__MODULE__, [])
  def init(_), do: {:ok, %{messages: 0}}
  
  def send_message(pid, msg), do: GenServer.cast(pid, {:message, msg})
  def get_count(pid), do: GenServer.call(pid, :get_count)
  
  def handle_cast({:message, _msg}, state) do
    {:noreply, %{state | messages: state.messages + 1}}
  end
  
  def handle_call(:get_count, _from, state) do
    {:reply, state.messages, state}
  end
end

{:ok, agent} = BenchAgent.start_link([])

Benchee.run(%{
  "genserver_cast" => fn ->
    BenchAgent.send_message(agent, %{type: "test", data: "benchmark"})
  end,
  "process_send" => fn ->
    send(agent, {:test_message, "benchmark"})
  end,
  "simple_message_creation" => fn ->
    %{
      from: "sender",
      to: "receiver", 
      type: "command",
      payload: %{action: "test", data: "benchmark"},
      timestamp: System.system_time()
    }
  end
}, time: 3, memory_time: 1)
'
fi

# Persistence benchmarks
if [ "$SUITE" = "all" ] || [ "$SUITE" = "persistence" ]; then
    run_benchmark "persistence" "Persistence Layer Benchmarks" '
# Benchmark data persistence operations
data_small = %{id: "test", value: 42}
data_medium = %{
  id: "test",
  agent_state: %{
    capabilities: ["read", "write", "execute"],
    metadata: %{created: System.system_time(), active: true}
  },
  history: Enum.map(1..100, fn i -> %{id: i, action: "test_#{i}"} end)
}

data_large = %{
  id: "large_test",
  agent_state: %{
    capabilities: Enum.map(1..1000, &"capability_#{&1}"),
    metadata: %{created: System.system_time(), active: true, data: String.duplicate("x", 1000)}
  },
  history: Enum.map(1..10000, fn i -> %{id: i, action: "test_#{i}", timestamp: System.system_time()} end)
}

Benchee.run(%{
  "serialize_small" => fn -> :erlang.term_to_binary(data_small) end,
  "serialize_medium" => fn -> :erlang.term_to_binary(data_medium) end,
  "serialize_large" => fn -> :erlang.term_to_binary(data_large) end,
  "json_encode_small" => fn -> Jason.encode!(data_small) end,
  "json_encode_medium" => fn -> Jason.encode!(data_medium) end,
  "ets_insert" => fn -> 
    table = :ets.new(:bench, [:set])
    :ets.insert(table, {:key, data_medium})
    :ets.delete(table)
  end
}, time: 3, memory_time: 1)
'
fi

# Calculate duration
end_time=$(date +%s)
duration=$((end_time - start_time))

echo -e "${GREEN}ğŸ‰ Benchmarks completed!${NC}"
echo -e "${BLUE}â±ï¸  Total duration: ${duration}s${NC}"

if [ "$SAVE_RESULTS" = true ]; then
    echo -e "${BLUE}ğŸ“ Results saved to: $OUTPUT_DIR${NC}"
fi

echo
echo -e "${YELLOW}ğŸ’¡ Benchmark tips:${NC}"
echo -e "  â€¢ Run benchmarks multiple times for consistency"
echo -e "  â€¢ Compare results across different hardware"
echo -e "  â€¢ Use --save --format html for detailed analysis"
echo -e "  â€¢ Monitor system load during benchmarks"
echo